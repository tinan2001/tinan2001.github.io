[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "About Me",
    "section": "",
    "text": "[insert picture of self]\nMy name is Tina Ni. I am currently a second year graduate student studying geographic information science at SUNY University at Buffalo. This website was created to showcase my past and ongoing projects that I have created that revolve around GIS and spatial data science."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "GEO653.html",
    "href": "GEO653.html",
    "title": "LCLU Change in College Town: Amherst, New York",
    "section": "",
    "text": "Methods. This project looked at the land use land cover change in Amherst, New York. This project utilized ESRI ArcGIS Pro to perform a supervised remote sensing of both X meter sentinal and X meter orthophotos. Classifications were originally categorizes as X,X,X, and X.\nClassification files were then analyzed using R programming to show temperal land change and also to identify areas of physical land change hotspots.\n\nLoad necessary packages\n\n\nCode\nlibrary(terra)\nlibrary(sf)\nlibrary(ggplot2)\nlibrary(leaflet)\nlibrary(sp)\nlibrary(raster)\nlibrary(dbscan)\nlibrary(dplyr)\nlibrary(osmdata)\nlibrary(ggspatial)\nlibrary(prettymapr)\nlibrary(kableExtra)\n\n\n\n\n\nImport classified land raster files and Amherst, NY town boundary\n\n\nCode\nr10 &lt;- rast(\"GEO653_documents/Amherst_2010.tif\") \nr15 &lt;- rast(\"GEO653_documents/Amherst_2015.tif\") \nr17 &lt;- rast(\"GEO653_documents/Amherst_2017.tif\") \nr20 &lt;- rast(\"GEO653_documents/Amherst_2020.tif\") \nr24 &lt;- rast(\"GEO653_documents/Amherst_2024.tif\") \n\namherst &lt;- vect(\"GEO653_documents/BB.shp\")\n\n\n\n####Reproject variables and clean up data\n\n\nCode\namherst &lt;- project(amherst, crs(r17))\namherst_sf &lt;- st_as_sf(amherst)\n\n#crop boundaries\nclip_raster &lt;- function(r, boundary) \n                                          {mask (crop (r, boundary[2, ]), boundary)}\n\nr10 &lt;- clip_raster(r10, amherst)\nr15 &lt;- clip_raster(r15, amherst)\nr17 &lt;- clip_raster(r17, amherst)\nr20 &lt;- clip_raster(r20, amherst)\nr24 &lt;- clip_raster(r24, amherst)\n\n\nrcl &lt;- matrix(c(\n  10, 1,\n  20, 2,\n  30, 3,\n  60, 4\n), ncol = 2, byrow = TRUE)\n\n\n\nr10_reclass &lt;- classify(r10, rcl)\nr15_reclass &lt;- classify(r15, rcl)\nr17_reclass &lt;- classify(r17, rcl)\nr20_reclass &lt;- classify(r20, rcl)\nr24_reclass &lt;- classify(r24, rcl)\n\nr10_aligned &lt;- r10_reclass\nr15_aligned &lt;- resample(r15_reclass, r10_reclass, method = \"near\")\nr17_aligned &lt;- resample(r17_reclass, r10_reclass, method = \"near\")\nr20_aligned &lt;- resample(r20_reclass, r10_reclass, method = \"near\")\nr24_aligned &lt;- resample(r24_reclass, r10_reclass, method = \"near\")\n\nr10_aligned[r10_aligned == 255] &lt;- NA\nr15_aligned[r15_aligned == 255] &lt;- NA\nr17_aligned[r17_aligned == 255] &lt;- NA\nr20_aligned[r20_aligned == 255] &lt;- NA\nr24_aligned[r24_aligned == 255] &lt;- NA\n\n\n\n\nPlot to show areas that changed from vegetation to developed urban between different years\n\n\nCode\npar(mfrow = c(1, 3), oma = c(0, 0, 4, 0)) \n\nveg_to_dev_10_15 &lt;- (r10_aligned %in% c(0,3)) & (r15_aligned %in% c(1,2))\nplot(veg_to_dev_10_15, main = \"2010 - 2015\")\n\nveg_to_dev_15_17 &lt;- (r15_aligned %in% c(0,3)) & (r17_aligned %in% c(1,2))\nplot(veg_to_dev_15_17, main = \"2015 - 2017\")\n\nveg_to_dev_17_20 &lt;- (r17_aligned %in% c(0,3)) & (r20_aligned %in% c(1,2))\nplot(veg_to_dev_17_20, main = \"2017 - 2020\")\n\nmtext(\"Areas of Vegetation to Developed\",\n      outer = TRUE, cex = 1.5, line = 2)\n\n\n\n\n\n\n\n\n\n\n\nCode\npar(mfrow = c(1, 1), oma = c(0, 0, 4, 0)) \nveg_to_dev_10_24 &lt;- (r10_aligned %in% c(0,3)) & (r24_aligned %in% c(1,2))\nplot(veg_to_dev_10_24, main = \"2010 - 2024\")\n\nveg_to_dev_17_24 &lt;- (r17_aligned %in% c(0,3)) & (r24_aligned %in% c(1,2))\n#plot(veg_to_dev_17_24, main = \"2017 - 2024\")\n\nveg_to_dev_20_24 &lt;- (r20_aligned %in% c(0,3)) & (r24_aligned %in% c(1,2))\n#plot(veg_to_dev_20_24, main = \"2020 - 2024\")\n\nmtext(\"Areas of Vegetation to Developed\",\n      outer = TRUE, cex = 1.5, line = 2)\n\n\n\n\n\n\n\n\n\n\n\nFind spatial clustering of land change (‘True’ cells)\n\n\nCode\nmoving_window &lt;- matrix(1, 5, 5)\n\n\n\n\n2017 - 2020 Hotspots\n\n\n2020 - 2024 Hotspots\n\n\n2010 - 2024 Hotspots\n\n\nCode\nveg_loss_hotspots_10_24 &lt;- focal(veg_to_dev_10_24, w = moving_window, fun = sum, na.policy = \"omit\")\nplot(veg_loss_hotspots_10_24, main = \"Vegetation to Development Hotspots 2010 - 2024\", col = terrain.colors(170))\n\n\n\n\n\n\n\n\n\nCode\nstrong_hotspots_10_24&lt;- veg_loss_hotspots_10_24 &gt; 15\nplot(strong_hotspots_10_24, main = \"Strong Vegetation Loss Hotspots 2010 - 2024\", col = c(\"white\", \"red\"))\n\n\n\n\n\n\n\n\n\nCode\nn_hotspot_pixels_10_24 &lt;- global(strong_hotspots_10_24, sum, na.rm = TRUE)\n\nhotspot_area_m2_10_24 &lt;- n_hotspot_pixels_10_24 * 900\n\nprint(paste(\"2010-2024 Total strong hotspot area (meters^2):\", round(hotspot_area_m2_10_24, 2)))\n\n\n[1] \"2010-2024 Total strong hotspot area (meters^2): 953100\"\n\n\n\n\n2017 - 2024 Hotspots\n\n\nCode\n#convert raster hotspots to polygons\nhotspot_polygons &lt;- as.polygons(strong_hotspots_10_24, dissolve = FALSE)\nhotspot_polygons &lt;- hotspot_polygons[hotspot_polygons[[1]] == 1, ]\n\n#find centroids and coords to polygons\ncentroids &lt;- centroids(hotspot_polygons)\ncoords &lt;- crds(centroids)\n\n#find clustering of centroids \neps_value &lt;- 50  # adjust as needed\nclust &lt;- dbscan(coords, eps = eps_value, minPts = 1)\ncentroids$cluster &lt;- clust$cluster\n\nattr_df &lt;- as.data.frame(centroids)\n\n#make grouped centroids into one single centroid by average (x,y)\ncentroids_df &lt;- cbind(attr_df, x = coords[,1], y = coords[,2])\n\ncluster_summary &lt;- centroids_df %&gt;%\n  group_by(cluster) %&gt;%\n  summarize(\n    mean_x = mean(x),\n    mean_y = mean(y),\n    count = n()\n  )\n\n\ncluster_centroids &lt;- vect(cluster_summary[, c(\"mean_x\", \"mean_y\")], \n                         geom = c(\"mean_x\", \"mean_y\"), crs = crs(centroids))\n#areas of hotspots\nplot(cluster_centroids)\n\n\n\n\n\n\n\n\n\n\n\nCode\n# --------------------------------------\n# 1. Convert to sf and reproject to WGS84\n# --------------------------------------\n# Assuming clustered_centroids is a SpatVector from terra\nclustered_centroids_sf &lt;- st_as_sf(cluster_centroids)\nclustered_centroids_wgs84 &lt;- st_transform(clustered_centroids_sf, 4326)\n\n# --------------------------------------\n# 2. Buffer each centroid (e.g., ~25m buffer in degrees)\n# --------------------------------------\nbuffer_radius &lt;- 0.00025  # ~25 meters in lat/lon degrees\ncentroid_buffers &lt;- st_buffer(clustered_centroids_wgs84, dist = buffer_radius)\n\n# --------------------------------------\n# 3. Query OSM for landuse features (one request for all)\n# --------------------------------------\n\n# Get bounding box of all buffers\nglobal_bbox &lt;- st_bbox(centroid_buffers)\n\n# Set OSM key to landuse\nosm_key &lt;- \"landuse\"\n\n# Build and send query\nquery &lt;- opq(bbox = global_bbox) %&gt;%\n  add_osm_feature(key = osm_key)\n\nosm_data &lt;- tryCatch(\n  osmdata_sf(query),\n  error = function(e) NULL\n)\n\n# --------------------------------------\n# 4. Intersect returned OSM features with centroid buffers\n# --------------------------------------\nif (!is.null(osm_data) && nrow(osm_data$osm_polygons) &gt; 0) {\n  osm_landuse &lt;- osm_data$osm_polygons\n  \n\n  overlaps &lt;- st_join(centroid_buffers, osm_landuse, join = st_intersects, left = FALSE)\n  \n  if (nrow(overlaps) &gt; 0) {\n    \n    cat(\"Matched landuse types:\\n\")\n    print(table(overlaps$landuse))\n\n   \n    plot(st_geometry(centroid_buffers), col = NA, border = \"gray\", main = \"Land Use Around Clustered Centroids\")\n    plot(st_geometry(osm_landuse), col = \"lightgreen\", border = \"green\", add = TRUE)\n    plot(st_geometry(clustered_centroids_wgs84), col = \"black\", pch = 19, add = TRUE)\n\n  } else {\n    cat(\"no landuse polygons overlapped with point buffer.\")\n  }\n} else {\n  cat(\"no landuse data returned from OSM.\")\n}\n\n\nMatched landuse types:\n\n  brownfield construction       meadow  residential \n           1            3            1           13 \n\n\n\n\n\n\n\n\n\n\n\nCode\n#buffer the centroids to find the various land use around the area\ncentroid_buffers$id &lt;- seq_len(nrow(centroid_buffers))\n\nlanduse_matches &lt;- st_join(centroid_buffers, osm_landuse, join = st_intersects, left = FALSE)\n\n#find the most dominant land use and assign to the centroid\ndominant_landuse &lt;- landuse_matches %&gt;%\n  st_drop_geometry() %&gt;%\n  group_by(id, landuse) %&gt;%\n  summarize(count = n(), .groups = \"drop\") %&gt;%\n  group_by(id) %&gt;%\n  slice_max(order_by = count, n = 1, with_ties = FALSE)\n\n\ncentroid_with_landuse &lt;- left_join(centroid_buffers, dominant_landuse, by = \"id\")\ncentroid_points &lt;- st_centroid(centroid_with_landuse)\n\ncentroid_points &lt;- centroid_points %&gt;%\n  mutate(\n    landuse = ifelse(is.na(landuse), \"not found\", landuse),\n    count = 1)\n\nggplot() +\n  annotation_map_tile(type = \"osm\") +\n  geom_sf(data = centroid_points, aes(color = landuse), size = 3, shape = 19) +\n  scale_color_viridis_d(option = \"plasma\", name = \"Dominant Landuse\") +\n  theme_minimal() +\n  labs(\n    title = \"Centroids Colored by Dominant Landuse\"\n  ) +\n  theme(legend.position = \"right\")\n\n\nZoom: 11\n\n\n\n\n\n\n\n\n\n\n\n2017 - 2024 Hotspots\n\n\n2010 - 2024 Hotspots\n\n\nCode\n#2010 - 2024 Hotspot \nstrong_hotspots_proj_10_24 &lt;- project(strong_hotspots_10_24, \"EPSG:3857\")\n\n#Convert True and False to 1 and 0\n\nstrong_hotspots_proj_10_24_numeric &lt;- as.numeric(strong_hotspots_proj_10_24)\nstrong_hotspots_proj_10_24_numeric_no_zeros &lt;- mask(strong_hotspots_proj_10_24_numeric, strong_hotspots_proj_10_24_numeric, maskvalues = c(0, NA))\n\n\n#Color the hotspots red and make the non hotspot areas transparent\n\npal_hotspots &lt;- colorNumeric(\n  palette = c(\"transparent\", \"red\"),\n  domain = c(0, 1)\n)\n\n#Plot on a leafet map with osm to identify land use of hotspot areas\nleaflet() %&gt;%\n  addProviderTiles(providers$OpenStreetMap) %&gt;%\n  addRasterImage(\n    strong_hotspots_proj_10_24_numeric_no_zeros,\n    colors = pal_hotspots,\n    opacity = 0.50,  # Show hotspots with full opacity\n    project = FALSE\n  ) %&gt;%\n  addLegend(\n    position = \"bottomright\",\n    pal = pal_hotspots,\n    values = c(0, 1),\n    title = \"Vegetation Loss Hotspots (2010-2024)\",\n    labels = c(\"No Hotspot\", \"Hotspot\")\n  )\n\n\n\n\n\n\n\n\nPhysically locate areas of hotspots and manually classfy land use based on the leafet map\n\n\nCode\nhotspots &lt;- read_sf(\"GEO653_documents/Hotspot_Points.shp\")\nUB &lt;- read_sf(\"GEO653_documents/UB.shp\")\namherst &lt;- read_sf(\"GEO653_documents/BB.shp\")\n\n\nnum_hotspots &lt;- length(hotspots)\n\n\ndf &lt;- as.data.frame(hotspots)\n\ndf &lt;- df %&gt;%\n  mutate(Name = ifelse(Name == \"Commerical\", \"Commercial\", Name)) %&gt;%\n  mutate(Name = ifelse(Name == \"Apartment\", \"Apartments\", Name))%&gt;%\n  mutate(Name = ifelse(Name == \"Residental\", \"Residential\", Name)) %&gt;%\n  group_by(Name) %&gt;%\n  summarize(Count = n())\n\nhotspots &lt;- hotspots %&gt;%\n  mutate(Name = ifelse(Name == \"Commerical\", \"Commercial\", Name)) %&gt;%\n  mutate(Name = ifelse(Name == \"Apartment\", \"Apartments\", Name)) %&gt;%\n  mutate(Name = ifelse(Name == \"Residental\", \"Residential\", Name))\n\nkable(df, caption = \" 2024 Hotspot Land Use\")%&gt;% \n  kable_styling(bootstrap_options = 'striped')\n\n\n\n2024 Hotspot Land Use\n\n\nName\nCount\n\n\n\n\nApartments\n8\n\n\nCommercial\n2\n\n\nFalse\n2\n\n\nHospital\n1\n\n\nIn Progress\n1\n\n\nMisc.\n1\n\n\nParking Lot\n2\n\n\nResidential\n11\n\n\nSolar\n1\n\n\n\n\n\n\n\nCode\namherst$Source &lt;- \"Amherst\"\nUB$Source &lt;- \"UB\"\nhotspots$Source &lt;- \"Hotspots\"\n\nub_utm &lt;- st_transform(UB, crs = 32618)\nbuffer_2mile &lt;- st_buffer(ub_utm, dist = 3218.68)\n\nggplot() +\n  geom_sf(data = amherst, aes(fill = Source), color = \"black\", alpha = 0.3) +\n  geom_sf(data = UB, aes(shape = Source), color = \"blue\", size = 4) +  # UB points\n  geom_sf(data = hotspots, aes(color = Name), size = 2, alpha = 0.8) +\n  geom_sf(data = buffer_2mile, fill = \"lightblue\", alpha = 0.5) +\n  scale_shape_manual(values = c(\"UB\" = 16)) + \n  scale_fill_manual(values = c(\"Amherst\" = \"lightgray\")) +\n  scale_color_manual(values = c(\n    \"Residential\" = \"red\",\n    \"Apartments\" = \"purple\",\n    \"Commercial\" = \"orange\",\n    \"Hospital\" = \"green\",\n    \"Parking Lot\" = \"brown\",\n    \"In Progress\" = \"pink\",\n    \"Solar\" = \"black\",\n    'Other' = 'gray'\n  )) +\n  theme_minimal() +\n  labs(\n    title = \"Hotspots in Amherst and UB Area\",\n    fill = \"Polygon Layer\",\n    color = \"Hotspot Type\",\n    \n    shape = \"UB Symbol\"\n  )\n\n\n\n\n\n\n\n\n\nCode\nhotspots_utm &lt;- st_transform(hotspots, crs = st_crs(buffer_2mile))\nhotspots_within_buffer &lt;- st_intersection(hotspots_utm, buffer_2mile)\n\ndf2 &lt;- as.data.frame( hotspots_within_buffer)\n\ndf2 &lt;- df2 %&gt;%\n  group_by(Name) %&gt;%\n  summarize(Count = n())\n\nkable(df2, caption = \"2024 Hotspot Land Use in 2 Mile Buffer of University at Buffalo\")%&gt;% \n  kable_styling(bootstrap_options = 'striped')\n\n\n\n2024 Hotspot Land Use in 2 Mile Buffer of University at Buffalo\n\n\nName\nCount\n\n\n\n\nApartments\n7\n\n\nCommercial\n1\n\n\nFalse\n2\n\n\nHospital\n1\n\n\nIn Progress\n1\n\n\nMisc.\n1\n\n\nParking Lot\n2\n\n\nResidential\n1"
  },
  {
    "objectID": "GEO559.html",
    "href": "GEO559.html",
    "title": "GEO559",
    "section": "",
    "text": "This is the GIS Environmental Modeling Project"
  },
  {
    "objectID": "resume.html",
    "href": "resume.html",
    "title": "Tina Ni Resume",
    "section": "",
    "text": "▪   ▪   ▪   ▪\n\n\n\nEDUCATION\n\nSUNY University at Buffalo  August 2024 – December 2025 (Expected)\n\nSecond Year Graduate Student - Master of Science in Geographic Information Science  Buffalo, NY\n▪ Cumulative GPA: 4.0/4.0\n▪ Current enrolled courses (Fall 2025): GIS and Machine Learning, Dynamic Modeling\n▪ Teaching Assistant for Global Climate Change and Maps: Earth from Above\n▪ Secretary for Geography Graduate Student Association\n\n\n\n\nSUNY University at Buffalo  February 2023\n\n Bachelor of Science, Environmental Geoscience Buffalo, NY\n▪ 3.76/4.0 GPA, summa cum laude\n▪ Teaching assistant for Ecological Methods\n\n\n\n\n\nWORK EXPERIENCE\n\nGeocove, Inc. (Part-time)   November 2024 – Current\n\nGIS Technician  Cheektowaga, NY\n▪ Refactored and debugged 23 deprecated R scripts generating updated geospatial datasets, integrating most recent U.S. Census and OpenStreetMap data to be used for a New York City MTA toolkit\n▪ Annotated over 1,000 emergency disaster-related tweets for a NSF-funded GeoAI research project focused on developing AI models to geolocate disaster areas for emergency management.\n▪ Converted Python 2.x geoprocessing scripts to Python 3.x to support the migration from ArcMap to ArcGIS Pro\n▪ Developed automation scripts utilizing ArcPy to perform routine data backups and archiving of AGOL and Portal features\n▪ Worked with ArcGIS Pro to update various types of vector data and facilitated the migration of old datasets to ESRI solutions.\n▪ Effectively communicated with public and private stakeholders to achieve better project alignment for goals, timelines, and deliverables.\n\n\n\nWatts Architecture & Engineers (Full-time)  March 2024 – November 2024\n\nAir Sampling Technician and Project Monitor/i&gt;  Buffalo, NY\n▪ Conducted environmental air sample tests for harmful regulated building material debris.\n▪ Communicated and addressed issues to contractors and clients regarding NYSDOL ICR-56\n▪ Recorded, analyzed, and utilized project records to create project closeout reports and deliverables for clients\n\n\n\nWNY Partnership for Regional Invasive Species (Seasonal Full-time)  May 2023 – September 2023\n\nInvasive Species Management Assistant  Buffalo, NY\n▪ Gathered invasive species data using ESRI Survey123 and Field Maps in the Western New York jurisdiction\n▪ Utilized iMapInvasive and ArcGIS Pro to create Management and Survey Reports for stakeholders\n\n\n\n\n\nSKILLS AND COURSES TAKEN\nSkills: Intermediate Python, Intermediate R, Beginner SQL, familiarity with ESRI products (ArcGIS Pro, ArcPy, Experience Builder, Field Maps, Survey123), Visual Basic, ImageJ, and ENVI\nCourses Taken: Spatial Data Science, Advance Remote Sensing, GIS in Environmental Modeling, Integrated Environmental Management, Environmental Engineering"
  },
  {
    "objectID": "Resume Awesome/Resume Awesome.html",
    "href": "Resume Awesome/Resume Awesome.html",
    "title": "",
    "section": "",
    "text": "Code\n\n\n\n\n\nSome stuff about me\n\nI poisoned myself doing research.\nI was the first woman to win a Nobel prize\nI was the first person and only woman to win a Nobel prize in two different sciences.\n\n\n\nEducation\n\n\n# A tibble: 3 × 5\n  what                  when    with                where          why      \n  &lt;chr&gt;                 &lt;chr&gt;   &lt;chr&gt;               &lt;chr&gt;          &lt;list&gt;   \n1 Informal studies      1889-91 Flying University   Warsaw, Poland &lt;chr [0]&gt;\n2 Master of Physics     1893    Sorbonne Université Paris, France  &lt;chr [0]&gt;\n3 Master of Mathematics 1894    Sorbonne Université Paris, France  &lt;chr [0]&gt;\n\n\n\n\nNobel Prizes\n\n\n# A tibble: 2 × 3\n  what                      when with                                           \n  &lt;glue&gt;                   &lt;dbl&gt; &lt;chr&gt;                                          \n1 Nobel Prize in Physics    1903 Awarded for her work on radioactivity with Pie…\n2 Nobel Prize in Chemistry  1911 Awarded for the discovery of radium and poloni…\n\n\n\n\nPublications"
  },
  {
    "objectID": "CV_Quarto/resume_alexV2.html",
    "href": "CV_Quarto/resume_alexV2.html",
    "title": "",
    "section": "",
    "text": "Code\n\n\n\n\n\n\n\n\n\n\nWorked on Meta’s marketing decision science team using causal inference methods. \n\n\n\nLed team (3 DS) to build a bayesian multi-touch attribution (PYMC) product measuring effectiveness of ad campaigns on brand lift surveys. Outputing lift curves, channel contribution charts, and a simulation tool for client delivery.\nRegularly deliver advanced analytical studies (diff-in-diff, dominance analysis, time series, MTA). In total, delivered over 100 of these studies to clients including Google, Meta, CVS, Chickfila, and AT&T.\n\n\n\nDeveloped and automated wave-over-wave chi-squared tests on time series data in 17 tracker surveys of 5 countries\nOver 300+ requests, pulled data from API or large database into R, wrangled data using R, and output figures and tables\nLed project to build a Python Web Bot (Selenium) to automate generation of test cases in surveys, contributed this to data science code base (used by 60+ data scientists)\n\n\n\nLed in modeling projects predicting election turnout for entire U.S. in 2022 analyzing over 460 million records.\nWrangled, cleaned, weighted, or made presentations for 60+ survey datasets with R, SQL, and AWS\nUsing R Shiny, built a codeless-crosstab tool for company’s research team\n\n\nsurvey_tools - Creator and Maintainer of an open-source Python package with convenient functions for creating weighted crosstabs, recoding variables, and weighting surveys. This package is available through pip and PyPI.\n\nLanguages and Tools - proficient: Python, R, Bash intermediate: Spark, SQL, Tensorflow/Keras, Git, Docker\nSelected Coursework - Deep Learning, Bayesian Machine Learning, Big Data Systems with Spark, Linear Models for Data Science, Statistical Learning, Data Engineering, Linear Algebra, Econometrics I & II, Data Visualization, Programming in Python\n\n\nEstimate daily a heirarchal bayesian regression model to generate probabilities of election outcomes using latest polls\nCreated a live data pipeline with Github Actions and Python &gt; Live Quarto Dashboard\n\n\nAwarded $1500; wrote, fielded, analyzed original survey research project with Prolific\nObservational study: answered research questions using multiple linear regression and multinomial logit regression."
  },
  {
    "objectID": "CV_Quarto/resume_alexV3.html",
    "href": "CV_Quarto/resume_alexV3.html",
    "title": "",
    "section": "",
    "text": "Code\n\n\n\n\n\n\nSummary: Data Scientist with graduate degree + 5 years of experience focusing on surveys, A/B tests, and causal inference\nLanguages: Python, R, SQL, Spark, Javascript (in this order)\n\n\nconstructed and reported on Long Term Holdout with power analysis, balance testing, and regular stats readouts.\nAnalyzed on ad incrementality via AB testing on On-Platform ads. Mainly Instagram.\nThis contract for Meta was through Tundra Technical Solutions \n\n\nImplemented “Connection Scores” in a marketing funnel showing which factors influence use of Meta AI.\nUsed on-platform A/B experiment data and marketing campaign survey data to measure and track user public affairs sentiment on a dashboard I built with a DE.\nThis contract for Meta was through Crystal Equation \n\n\n\nAnalyzed A/B Tests (and Diff-in-Diff at times) to measure marketing incrementality in over 40 studies on the Meta, Google, and CVS accounts.\nLed team (3 DS) to build a bayesian multi-touch attribution (PYMC) product measuring effectiveness of ad campaigns on brand lift surveys by channel. Outputing lift curves, channel contribution charts, and a simulation tool.\n\n\n\nDeveloped and automated wave-over-wave chi-squared tests on time series data in 17 tracker surveys of 5 countries\nOver 300+ requests, pulled data from API or large database into R, wrangled data using R, and output figures and tables\n\n\n\nLed in modeling projects predicting election turnout for entire U.S. in 2022 analyzing over 460 million records.\nWrangled, cleaned, weighted, or made presentations for 60+ survey datasets with R, SQL, and AWS\n\n\nsurvey_tools - Creator and Maintainer of an open-source python package for survey researchers with convenient functions for weighted crosstabs, recoding variables, and weighting surveys. This package is available on github [here] and PyPI [here].\n\n\n\n\nEstimate daily a heirarchal bayesian beta regression model to generate probabilities of election outcomes using latest polls\nCreated a live data pipeline with Github Actions and Python &gt; Live Quarto Dashboard\n\n\nSolved entity resolution problem linking unstructured, sparse wikipedia references to Internet Archive API Database References.\nImplemented model in Python which, given a wikipedia reference, will return a match in the Internet Archive Database along with a probability of matching."
  },
  {
    "objectID": "CV_Quarto/resume_alex.html",
    "href": "CV_Quarto/resume_alex.html",
    "title": "",
    "section": "",
    "text": "Code\n\n\n\n\n\n\n\n\n\nLanguages and Tools - proficient: Python, R, Bash intermediate: Spark, SQL, Tensorflow/Keras, Git, Docker\nSelected Coursework - Deep Learning, Bayesian Machine Learning, Big Data Systems with Spark, Linear Models for Data Science, Statistical Learning, Data Engineering, Linear Algebra, Econometrics I & II, Data Visualization, Programming in Python\n\n\nDelivered 30+ advanced analytical studies (MTA, drivers analysis, diff-in-diff) to high profile clients including 15+ to marketing teams at Google and Meta\nBuilt diff-in-diff and multi-touch attribution model codebase in Python(OOP) for Ad Data Science Team. This codebase has saved 100+ hours and brought over in $2 million in revenue to date\n\n\n\nSolved entity resolution problem with Gradient Boosted Trees linking sparse wikipedia references to Internet Archive Database References\nImplemented model in Python which, given a wikipedia reference, will return a match in the Internet Archive Database along with a probability of matching.\n\n\n\nDeveloped and automated wave-over-wave statistical tests on time series data in 17 tracker surveys of 5 countries\nLed project to build a Python Web Bot (Selenium) to automate generation of test cases in surveys, contributed this to data science code base (used by 60+ data scientists)\nOver 300+ requests, pulled data from API or large database into R, wrangled data using R, and output figures and tables\n\n\n\nLed in modeling projects predicting election turnout for entire U.S. in 2022, phone response rates, etc.\nWrangled, cleaned, weighted, or made presentations for 60+ survey datasets with R, SQL, and AWS\nUsing R Shiny, built a codeless-crosstab tool for company’s research team\n\n\n\nDesigned and executed multiple survey experiments in original research projects\nMentored 60+ students in solving econometrics problems in weekly office hours\nVisualized data using R creating 50+ informative figures for AFS official report , news outlets, and professor’s projects\n\n\n\nCompared and estimated several Bayesian Regression models with PYMC3 in Python. Ultimately, used hierarchal negative binomial model to predict shootings and make inferences about gun laws\n\n\n\nrecieved award from Political Science department to fund a panel survey (n=1000).\nestimated linear regression and multinomial logit in observational study context and wrote paper"
  },
  {
    "objectID": "Resume/Resume.html",
    "href": "Resume/Resume.html",
    "title": "Tina Ni Resume",
    "section": "",
    "text": "▪   ▪   ▪   ▪\n\n\n\nEDUCATION\n\nSUNY University at Buffalo  August 2024 – December 2025 (Expected)\n\nSecond Year Graduate Student - Master of Science in Geographic Information Science  Buffalo, NY\n▪ Cumulative GPA: 4.0/4.0\n▪ Current enrolled courses (Fall 2025): GIS and Machine Learning, Dynamic Modeling\n▪ Teaching Assistant for Global Climate Change and Maps: Earth from Above\n▪ Secretary for Geography Graduate Student Association\n\n\n\n\nSUNY University at Buffalo  February 2023\n\n Bachelor of Science, Environmental Geoscience Buffalo, NY\n▪ 3.76/4.0 GPA, summa cum laude\n▪ Teaching assistant for Ecological Methods\n\n\n\n\n\nWORK EXPERIENCE\n\nGeocove, Inc. (Part-time)   November 2024 – Current\n\nGIS Technician  Cheektowaga, NY\n▪ Refactored and debugged 23 deprecated R scripts generating updated geospatial datasets, integrating most recent U.S. Census and OpenStreetMap data to be used for a New York City MTA toolkit\n▪ Annotated over 1,000 emergency disaster-related tweets for a NSF-funded GeoAI research project focused on developing AI models to geolocate disaster areas for emergency management.\n▪ Converted Python 2.x geoprocessing scripts to Python 3.x to support the migration from ArcMap to ArcGIS Pro\n▪ Developed automation scripts utilizing ArcPy to perform routine data backups and archiving of AGOL and Portal features\n▪ Worked with ArcGIS Pro to update various types of vector data and facilitated the migration of old datasets to ESRI solutions.\n▪ Effectively communicated with public and private stakeholders to achieve better project alignment for goals, timelines, and deliverables.\n\n\n\nWatts Architecture & Engineers (Full-time)  March 2024 – November 2024\n\nAir Sampling Technician and Project Monitor/i&gt;  Buffalo, NY\n▪ Conducted environmental air sample tests for harmful regulated building material debris.\n▪ Communicated and addressed issues to contractors and clients regarding NYSDOL ICR-56\n▪ Recorded, analyzed, and utilized project records to create project closeout reports and deliverables for clients\n\n\n\nWNY Partnership for Regional Invasive Species (Seasonal Full-time)  May 2023 – September 2023\n\nInvasive Species Management Assistant  Buffalo, NY\n▪ Gathered invasive species data using ESRI Survey123 and Field Maps in the Western New York jurisdiction\n▪ Utilized iMapInvasive and ArcGIS Pro to create Management and Survey Reports for stakeholders\n\n\n\n\n\nSKILLS AND COURSES TAKEN\nSkills: Intermediate Python, Intermediate R, Beginner SQL, familiarity with ESRI products (ArcGIS Pro, ArcPy, Experience Builder, Field Maps, Survey123), Visual Basic, ImageJ, and ENVI\nCourses Taken: Spatial Data Science, Advance Remote Sensing, GIS in Environmental Modeling, Integrated Environmental Management, Environmental Engineering"
  },
  {
    "objectID": "Untitled/Untitled.html",
    "href": "Untitled/Untitled.html",
    "title": "CV",
    "section": "",
    "text": "I poisoned myself doing research.\nI was the first woman to win a Nobel prize\nI was the first person and only woman to win a Nobel prize in two different sciences."
  },
  {
    "objectID": "Untitled/Untitled.html#some-stuff-about-me",
    "href": "Untitled/Untitled.html#some-stuff-about-me",
    "title": "CV",
    "section": "",
    "text": "I poisoned myself doing research.\nI was the first woman to win a Nobel prize\nI was the first person and only woman to win a Nobel prize in two different sciences."
  },
  {
    "objectID": "Untitled/Untitled.html#education",
    "href": "Untitled/Untitled.html#education",
    "title": "CV",
    "section": "Education",
    "text": "Education\n\n\n# A tibble: 3 × 5\n  what                  when    with                where          why      \n  &lt;chr&gt;                 &lt;chr&gt;   &lt;chr&gt;               &lt;chr&gt;          &lt;list&gt;   \n1 Informal studies      1889-91 Flying University   Warsaw, Poland &lt;chr [0]&gt;\n2 Master of Physics     1893    Sorbonne Université Paris, France  &lt;chr [0]&gt;\n3 Master of Mathematics 1894    Sorbonne Université Paris, France  &lt;chr [0]&gt;"
  },
  {
    "objectID": "Untitled/Untitled.html#nobel-prizes",
    "href": "Untitled/Untitled.html#nobel-prizes",
    "title": "CV",
    "section": "Nobel Prizes",
    "text": "Nobel Prizes\n\n\n# A tibble: 2 × 3\n  what                      when with                                           \n  &lt;glue&gt;                   &lt;dbl&gt; &lt;chr&gt;                                          \n1 Nobel Prize in Physics    1903 Awarded for her work on radioactivity with Pie…\n2 Nobel Prize in Chemistry  1911 Awarded for the discovery of radium and poloni…"
  },
  {
    "objectID": "Untitled/Untitled.html#publications",
    "href": "Untitled/Untitled.html#publications",
    "title": "CV",
    "section": "Publications",
    "text": "Publications"
  }
]